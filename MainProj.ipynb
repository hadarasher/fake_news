{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b37c070d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from bs4 import BeautifulSoup  \n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import numpy as np\n",
    "import requests\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1ae6d98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_soup_object(html_file_name):\n",
    "    page=requests.get(html_file_name)\n",
    "    soup=BeautifulSoup(page.content,'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "ba8db706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_list_to_file(file_name,list_name):\n",
    "    with open(file_name, 'w') as f:\n",
    "        for li in list_name:\n",
    "            f.write(\"%s\\n\" % li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e0458eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scapping articles'urls from the website\n",
    "#scrapping from multiple pages with 'loas more button' javascript link\n",
    "#used stackoverflow - https://stackoverflow.com/questions/68792444/how-to-scrape-website-if-it-has-load-more-button-to-load-more-content-on-the-pag\n",
    "def israel365news_crawler(webdriver_path,max_links):\n",
    "    driver=webdriver.Chrome(executable_path=webdriver_path)\n",
    "    soup=load_soup_object('https://www.israel365news.com/israel-news/')\n",
    "\n",
    "    try:\n",
    "        driver.maximize_window() #ensures that the window occupies the entire screen.\n",
    "        driver.implicitly_wait(10) #wait for up to 10 seconds for the element to appear before throwing an exception\n",
    "        driver.get(\"https://www.israel365news.com/israel-news/\")\n",
    "\n",
    "        urls=[]\n",
    "\n",
    "        while max_links>=0:\n",
    "            if max_links%100==0: print(f'{max_links} urls until finish')\n",
    "            #articles = driver.find_elements_by_css_selector(\"h1.elementor-heading-title.elementor-size-default\")\n",
    "            #articles=driver.find_elements(\"xpath\", \"//h1[@class='elementor-heading-title elementor-size-default']\")\n",
    "            articles=soup('h1',attrs={\"class\":\"elementor-heading-title elementor-size-default\"})\n",
    "\n",
    "            for art in articles:\n",
    "                #url=art.find_element_by_tag_name(\"a\").get_attribute(\"href\")\n",
    "                #url = art.find_element(\"xpath\", \"./a\").get_attribute(\"href\")\n",
    "                #urls.append(url)\n",
    "                #print(f'url appended: {url}')\n",
    "                a=art.find('a')\n",
    "                try:\n",
    "                    if 'href' in a.attrs:\n",
    "                        url=a.get('href')\n",
    "                        urls.append(url)\n",
    "                        #print(f'url appended: {url}')\n",
    "                        max_links-=1\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            #loadmore = driver.find_element_by_css_selector(\"a.elementor-button-link.elementor-button\")\n",
    "            loadmore=driver.find_element(\"xpath\", \"//a[contains(@class, 'elementor-button-link') and contains(@class, 'elementor-button')]\")\n",
    "\n",
    "            if loadmore.is_displayed():\n",
    "                driver.execute_script(\"arguments[0].click();\", loadmore)\n",
    "                time.sleep(5)\n",
    "            else:\n",
    "                print(\"no more articles to load\")\n",
    "                break\n",
    "\n",
    "    except StaleElementReferenceException:\n",
    "        pass\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    return urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "602ad213",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hadar\\AppData\\Local\\Temp\\ipykernel_6828\\2713619630.py:5: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver=webdriver.Chrome(executable_path=webdriver_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 urls until finish\n",
      "9900 urls until finish\n",
      "9800 urls until finish\n",
      "9700 urls until finish\n",
      "9600 urls until finish\n",
      "9500 urls until finish\n",
      "9400 urls until finish\n",
      "9300 urls until finish\n",
      "9200 urls until finish\n",
      "9100 urls until finish\n",
      "9000 urls until finish\n",
      "8900 urls until finish\n",
      "8800 urls until finish\n",
      "8700 urls until finish\n",
      "8600 urls until finish\n",
      "8500 urls until finish\n",
      "8400 urls until finish\n",
      "8300 urls until finish\n",
      "8200 urls until finish\n",
      "8100 urls until finish\n",
      "8000 urls until finish\n",
      "7900 urls until finish\n",
      "7800 urls until finish\n",
      "7700 urls until finish\n",
      "7600 urls until finish\n",
      "7500 urls until finish\n",
      "7400 urls until finish\n",
      "7300 urls until finish\n",
      "7200 urls until finish\n",
      "7100 urls until finish\n",
      "7000 urls until finish\n",
      "6900 urls until finish\n",
      "6800 urls until finish\n",
      "6700 urls until finish\n",
      "6600 urls until finish\n",
      "6500 urls until finish\n",
      "6400 urls until finish\n",
      "6300 urls until finish\n",
      "6200 urls until finish\n",
      "6100 urls until finish\n",
      "6000 urls until finish\n",
      "5900 urls until finish\n",
      "5800 urls until finish\n",
      "5700 urls until finish\n",
      "5600 urls until finish\n",
      "5500 urls until finish\n",
      "5400 urls until finish\n",
      "5300 urls until finish\n",
      "5200 urls until finish\n",
      "5100 urls until finish\n",
      "5000 urls until finish\n",
      "4900 urls until finish\n",
      "4800 urls until finish\n",
      "4700 urls until finish\n",
      "4600 urls until finish\n",
      "4500 urls until finish\n",
      "4400 urls until finish\n",
      "4300 urls until finish\n",
      "4200 urls until finish\n",
      "4100 urls until finish\n",
      "4000 urls until finish\n",
      "3900 urls until finish\n",
      "3800 urls until finish\n",
      "3700 urls until finish\n",
      "3600 urls until finish\n",
      "3500 urls until finish\n",
      "3400 urls until finish\n",
      "3300 urls until finish\n",
      "3200 urls until finish\n",
      "3100 urls until finish\n",
      "3000 urls until finish\n",
      "2900 urls until finish\n",
      "2800 urls until finish\n",
      "2700 urls until finish\n",
      "2600 urls until finish\n",
      "2500 urls until finish\n",
      "2400 urls until finish\n",
      "2300 urls until finish\n",
      "2200 urls until finish\n",
      "2100 urls until finish\n",
      "2000 urls until finish\n",
      "1900 urls until finish\n",
      "1800 urls until finish\n",
      "1700 urls until finish\n",
      "1600 urls until finish\n",
      "1500 urls until finish\n",
      "1400 urls until finish\n",
      "1300 urls until finish\n",
      "1200 urls until finish\n",
      "1100 urls until finish\n",
      "1000 urls until finish\n",
      "900 urls until finish\n",
      "800 urls until finish\n",
      "700 urls until finish\n",
      "600 urls until finish\n",
      "500 urls until finish\n",
      "400 urls until finish\n",
      "300 urls until finish\n",
      "200 urls until finish\n",
      "100 urls until finish\n",
      "0 urls until finish\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'https://www.israel365news.com/israel-news/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6828\\3322314414.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mwebdriver_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mr\"C:\\Users\\hadar\\Documents\\chromedriver_win32\\chromedriver.exe\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0murls_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0misrael365news_crawler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwebdriver_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mwrite_list_to_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"https://www.israel365news.com/israel-news/\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0murls_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6828\\1931536750.py\u001b[0m in \u001b[0;36mwrite_list_to_file\u001b[1;34m(file_name, list_name)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mwrite_list_to_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlist_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mli\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist_name\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mli\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: 'https://www.israel365news.com/israel-news/'"
     ]
    }
   ],
   "source": [
    "webdriver_path=r\"C:\\Users\\hadar\\Documents\\chromedriver_win32\\chromedriver.exe\"\n",
    "urls_list=israel365news_crawler(webdriver_path,10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7fcd615f",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_list_to_file('israel365news_urls.txt',urls_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "96b84296",
   "metadata": {},
   "outputs": [],
   "source": [
    "#crawl through all articles from Israel365News we've collected\n",
    "def load_articles_to_df_israel365news(urls_list):\n",
    "    headline=[]\n",
    "    writers=[]\n",
    "    date=[]\n",
    "    genre=[]\n",
    "    content=[]\n",
    "    i=0\n",
    "    \n",
    "    for url in urls_list:\n",
    "        soup=load_soup_object(url)\n",
    "        headline.append(soup.find(\"h1\",attrs={\"class\":\"elementor-heading-title elementor-size-default\"}).get_text().strip())\n",
    "        writers.append(soup.find(\"h4\",attrs={\"class\":\"elementor-author-box__name\"}).get_text().strip())\n",
    "        date.append(soup.find(\"div\",attrs={\"class\":\"elementor-element elementor-element-8f04d6d elementor-widget__width-auto elementor-widget elementor-widget-heading\"}).find(\"h2\").get_text().strip())\n",
    "        genre.append(\", \".join(item.get_text().strip() for item in soup.find(\"div\",attrs={\"class\":\"elementor-element elementor-element-1a40dbf elementor-widget__width-auto text-hover elementor-widget elementor-widget-heading\"}).find_all(\"a\")))\n",
    "        content.append(soup.find(\"div\",attrs={\"class\":\"elementor-element elementor-element-2106a85 elementor-widget elementor-widget-theme-post-content\"}).get_text().strip()) \n",
    "        i+=1\n",
    "        if(i%100==0):\n",
    "            print(f'{i} articles scrapped')\n",
    "\n",
    "    df=pd.DataFrame({\"headline\":headline,\"writers\":writers,\"date\":date,\"genre\":genre,\"content\":content})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "811d60d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 articles scrapped\n",
      "200 articles scrapped\n",
      "300 articles scrapped\n",
      "400 articles scrapped\n",
      "500 articles scrapped\n",
      "600 articles scrapped\n",
      "700 articles scrapped\n",
      "800 articles scrapped\n",
      "900 articles scrapped\n",
      "1000 articles scrapped\n",
      "1100 articles scrapped\n",
      "1200 articles scrapped\n",
      "1300 articles scrapped\n",
      "1400 articles scrapped\n",
      "1500 articles scrapped\n",
      "1600 articles scrapped\n",
      "1700 articles scrapped\n",
      "1800 articles scrapped\n",
      "1900 articles scrapped\n",
      "2000 articles scrapped\n",
      "2100 articles scrapped\n",
      "2200 articles scrapped\n",
      "2300 articles scrapped\n",
      "2400 articles scrapped\n",
      "2500 articles scrapped\n",
      "2600 articles scrapped\n",
      "2700 articles scrapped\n",
      "2800 articles scrapped\n",
      "2900 articles scrapped\n",
      "3000 articles scrapped\n",
      "3100 articles scrapped\n",
      "3200 articles scrapped\n",
      "3300 articles scrapped\n",
      "3400 articles scrapped\n",
      "3500 articles scrapped\n",
      "3600 articles scrapped\n",
      "3700 articles scrapped\n",
      "3800 articles scrapped\n",
      "3900 articles scrapped\n",
      "4000 articles scrapped\n",
      "4100 articles scrapped\n",
      "4200 articles scrapped\n",
      "4300 articles scrapped\n",
      "4400 articles scrapped\n",
      "4500 articles scrapped\n",
      "4600 articles scrapped\n",
      "4700 articles scrapped\n",
      "4800 articles scrapped\n",
      "4900 articles scrapped\n",
      "5000 articles scrapped\n",
      "5100 articles scrapped\n",
      "5200 articles scrapped\n",
      "5300 articles scrapped\n",
      "5400 articles scrapped\n",
      "5500 articles scrapped\n",
      "5600 articles scrapped\n",
      "5700 articles scrapped\n",
      "5800 articles scrapped\n",
      "5900 articles scrapped\n",
      "6000 articles scrapped\n",
      "6100 articles scrapped\n",
      "6200 articles scrapped\n",
      "6300 articles scrapped\n",
      "6400 articles scrapped\n",
      "6500 articles scrapped\n",
      "6600 articles scrapped\n",
      "6700 articles scrapped\n",
      "6800 articles scrapped\n",
      "6900 articles scrapped\n",
      "7000 articles scrapped\n",
      "7100 articles scrapped\n",
      "7200 articles scrapped\n",
      "7300 articles scrapped\n",
      "7400 articles scrapped\n",
      "7500 articles scrapped\n",
      "7600 articles scrapped\n",
      "7700 articles scrapped\n",
      "7800 articles scrapped\n",
      "7900 articles scrapped\n",
      "8000 articles scrapped\n",
      "8100 articles scrapped\n",
      "8200 articles scrapped\n",
      "8300 articles scrapped\n",
      "8400 articles scrapped\n",
      "8500 articles scrapped\n",
      "8600 articles scrapped\n",
      "8700 articles scrapped\n",
      "8800 articles scrapped\n",
      "8900 articles scrapped\n",
      "9000 articles scrapped\n",
      "9100 articles scrapped\n",
      "9200 articles scrapped\n",
      "9300 articles scrapped\n",
      "9400 articles scrapped\n",
      "9500 articles scrapped\n",
      "9600 articles scrapped\n",
      "9700 articles scrapped\n",
      "9800 articles scrapped\n",
      "9900 articles scrapped\n",
      "10000 articles scrapped\n"
     ]
    }
   ],
   "source": [
    "df=load_articles_to_df_israel365news(urls_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8d40e354",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('israel365news.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "bd9de4be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>writers</th>\n",
       "      <th>date</th>\n",
       "      <th>genre</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Knesset passes 2023-2024 state budget</td>\n",
       "      <td>JNS</td>\n",
       "      <td>May 24, 2023</td>\n",
       "      <td>Israel News</td>\n",
       "      <td>Israel’s Knesset on Wednesday passed the state...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Will flights from Israel to Mecca spark Temple...</td>\n",
       "      <td>Adam Eliyahu Berkowitz</td>\n",
       "      <td>May 23, 2023</td>\n",
       "      <td>Israel News</td>\n",
       "      <td>Prime Minister Netanyahu was in direct contact...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rabbi Leo Dee weighs $1.3b. suit against Amanpour</td>\n",
       "      <td>JNS</td>\n",
       "      <td>May 23, 2023</td>\n",
       "      <td>Israel News</td>\n",
       "      <td>Rabbi Leo Dee is considering suing CNN chief i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Israeli Cabinet boosts funds for Kotel infrast...</td>\n",
       "      <td>JNS</td>\n",
       "      <td>May 22, 2023</td>\n",
       "      <td>Israel News</td>\n",
       "      <td>The Israeli Cabinet on Sunday approved funding...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ben-Gvir visits Temple Mount: ‘We are the owne...</td>\n",
       "      <td>JNS</td>\n",
       "      <td>May 21, 2023</td>\n",
       "      <td>Israel News</td>\n",
       "      <td>During a visit to the Temple Mount in Jerusale...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10015</th>\n",
       "      <td>Israeli forces kill two terrorists in raid nea...</td>\n",
       "      <td>JNS</td>\n",
       "      <td>May 10, 2023</td>\n",
       "      <td>Israel News, Judea and Samaria</td>\n",
       "      <td>Israeli forces killed two Palestinian terroris...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10016</th>\n",
       "      <td>IDF eliminates three top Palestinian Islamic J...</td>\n",
       "      <td>JNS</td>\n",
       "      <td>May 9, 2023</td>\n",
       "      <td>Israel News, Terror Watch</td>\n",
       "      <td>The Israel Defense Forces killed three top Pal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10017</th>\n",
       "      <td>After six years, Israel razes illegal PA struc...</td>\n",
       "      <td>JNS</td>\n",
       "      <td>May 8, 2023</td>\n",
       "      <td>Israel News</td>\n",
       "      <td>Israel’s Civil Administration demolished a Pal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10018</th>\n",
       "      <td>Jerusalem Day: Mass return to the Temple Mount...</td>\n",
       "      <td>Adam Eliyahu Berkowitz</td>\n",
       "      <td>May 7, 2023</td>\n",
       "      <td>Israel News</td>\n",
       "      <td>Temple Mount advocacy groups are planning some...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10019</th>\n",
       "      <td>Netanyahu: Israel’s counterterror ops an ‘inte...</td>\n",
       "      <td>JNS</td>\n",
       "      <td>May 7, 2023</td>\n",
       "      <td>Israel News</td>\n",
       "      <td>Israeli Prime Minister Benjamin Netanyahu on S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10020 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                headline  \\\n",
       "0                  Knesset passes 2023-2024 state budget   \n",
       "1      Will flights from Israel to Mecca spark Temple...   \n",
       "2      Rabbi Leo Dee weighs $1.3b. suit against Amanpour   \n",
       "3      Israeli Cabinet boosts funds for Kotel infrast...   \n",
       "4      Ben-Gvir visits Temple Mount: ‘We are the owne...   \n",
       "...                                                  ...   \n",
       "10015  Israeli forces kill two terrorists in raid nea...   \n",
       "10016  IDF eliminates three top Palestinian Islamic J...   \n",
       "10017  After six years, Israel razes illegal PA struc...   \n",
       "10018  Jerusalem Day: Mass return to the Temple Mount...   \n",
       "10019  Netanyahu: Israel’s counterterror ops an ‘inte...   \n",
       "\n",
       "                      writers          date                           genre  \\\n",
       "0                         JNS  May 24, 2023                     Israel News   \n",
       "1      Adam Eliyahu Berkowitz  May 23, 2023                     Israel News   \n",
       "2                         JNS  May 23, 2023                     Israel News   \n",
       "3                         JNS  May 22, 2023                     Israel News   \n",
       "4                         JNS  May 21, 2023                     Israel News   \n",
       "...                       ...           ...                             ...   \n",
       "10015                     JNS  May 10, 2023  Israel News, Judea and Samaria   \n",
       "10016                     JNS   May 9, 2023       Israel News, Terror Watch   \n",
       "10017                     JNS   May 8, 2023                     Israel News   \n",
       "10018  Adam Eliyahu Berkowitz   May 7, 2023                     Israel News   \n",
       "10019                     JNS   May 7, 2023                     Israel News   \n",
       "\n",
       "                                                 content  \n",
       "0      Israel’s Knesset on Wednesday passed the state...  \n",
       "1      Prime Minister Netanyahu was in direct contact...  \n",
       "2      Rabbi Leo Dee is considering suing CNN chief i...  \n",
       "3      The Israeli Cabinet on Sunday approved funding...  \n",
       "4      During a visit to the Temple Mount in Jerusale...  \n",
       "...                                                  ...  \n",
       "10015  Israeli forces killed two Palestinian terroris...  \n",
       "10016  The Israel Defense Forces killed three top Pal...  \n",
       "10017  Israel’s Civil Administration demolished a Pal...  \n",
       "10018  Temple Mount advocacy groups are planning some...  \n",
       "10019  Israeli Prime Minister Benjamin Netanyahu on S...  \n",
       "\n",
       "[10020 rows x 5 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "e674cbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making crawler for Middle East Monitor website\n",
    "def load_articles_url_MEM(n_pages):\n",
    "    soup=load_soup_object(r'https://www.middleeastmonitor.com/category/news/middle-east/')\n",
    "    articles=soup(\"li\")\n",
    "    urls=[]\n",
    "    \n",
    "    for page in range(n_pages):\n",
    "        for art in articles:\n",
    "            a=art.find('a')\n",
    "            try:\n",
    "                if 'href' in a.attrs:\n",
    "                    url=a.get('href')\n",
    "                    urls.append(url)\n",
    "            except:\n",
    "                continue\n",
    "        next_page=soup.find(\"a\",attrs={\"class\":\"next page-numbers\"})[\"href\"]\n",
    "        soup=load_soup_object(next_page)\n",
    "        articles=soup(\"li\")\n",
    "        if(page%50==0): \n",
    "            print(f'{page} pages out of 350')\n",
    "        \n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "21df0725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 pages out of 350\n",
      "50 pages out of 350\n",
      "100 pages out of 350\n",
      "150 pages out of 350\n",
      "200 pages out of 350\n",
      "250 pages out of 350\n",
      "300 pages out of 350\n"
     ]
    }
   ],
   "source": [
    "mem_urls_list=load_articles_url_MEM(350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "2668f4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing links to other sites from list\n",
    "for url in mem_urls_list:\n",
    "    if(url.startswith(\"https://www.middleeastmonitor.com\")==False):\n",
    "        mem_urls_list.remove(url)\n",
    "write_list_to_file('mem_urls.txt',mem_urls_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab7a404",
   "metadata": {},
   "outputs": [],
   "source": [
    "#crawl through all articles from Israel365News we've collected\n",
    "def load_articles_to_df_mem(urls_list):\n",
    "    headline=[]\n",
    "    writers=[]#there is no authors declared inside the articles, will be filled with Unknown\n",
    "    date=[]\n",
    "    genre=[]\n",
    "    content=[]\n",
    "    i=0\n",
    "    \n",
    "    for url in urls_list:\n",
    "        try:\n",
    "            soup=load_soup_object(url)\n",
    "            headline.append(soup.find(\"div\",attrs={\"id\":\"post-page-title\"}).get_text().strip())\n",
    "            writers.append(\"Unknown\")\n",
    "            date.append(soup.find(\"span\",attrs={\"class\":\"post-page-date\"}).get_text().strip())\n",
    "            genre.append(\", \".join(item.get_text().strip() for item in soup.find(\"div\",attrs={\"class\":\"post-meta-top\"}).find_all(\"a\")))\n",
    "            content.append((\" \".join(item.get_text().strip() for item in soup.find(\"div\",attrs={\"id\":\"post-content\"}).find_all(\"p\"))))\n",
    "            i+=1\n",
    "            if(i%100==0):\n",
    "                print(f'{i} articles scrapped')\n",
    "            if(i>=10000):\n",
    "                break\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    df=pd.DataFrame({\"headline\":headline,\"writers\":writers,\"date\":date,\"genre\":genre,\"content\":content})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07ca18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mem=load_articles_to_df_mem(mem_urls_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e15cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mem.to_csv('mem.csv', index=False)\n",
    "df_mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a86758f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
